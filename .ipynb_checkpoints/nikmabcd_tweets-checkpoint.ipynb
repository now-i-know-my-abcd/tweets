{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b524377-4625-411e-a709-fd101d1f5f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-4.12.1-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting oauthlib<4,>=3.2.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib<2,>=1.2.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from tweepy) (2.28.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy) (2022.12.7)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.2.2 requests-oauthlib-1.3.1 tweepy-4.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tweepy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61d9a52-6363-48d6-8109-4f6c6c577866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snscrape\n",
      "  Downloading snscrape-0.5.0.20230113-py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from snscrape) (4.11.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: requests[socks] in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from snscrape) (2.28.2)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.9.2-cp38-cp38-macosx_10_15_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from snscrape) (2022.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from beautifulsoup4->snscrape) (2.3.2.post1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests[socks]->snscrape) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests[socks]->snscrape) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests[socks]->snscrape) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests[socks]->snscrape) (2022.12.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Installing collected packages: lxml, filelock, snscrape\n",
      "Successfully installed filelock-3.9.0 lxml-4.9.2 snscrape-0.5.0.20230113\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ea16f3-47e6-4527-8a32-42942419fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tweepy\n",
    "\n",
    "import tweepy as tw\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# your Twitter API key and API secret\n",
    "\n",
    "consumer_key = \"XXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "consumer_secret = \"XXXXXXXXXXXXXXXXXXXXX\"\n",
    "access_key = \"XXXXXXXXXXXXXXXXX\"\n",
    "access_secret = \"XXXXXXXXXXXX\"\n",
    "\n",
    "# authenticate\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tw.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "538f210b-3407-4de0-b894-5b600f65fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query1 = \"#ABCDstudy -filter:retweets\"\n",
    "search_query2 = \"ABCD study -filter:retweets\"\n",
    "search_query3 = \"Adolescent Brain Cognitive Development study -filter:retweets\"\n",
    "search_query4 = \"Adolescent Brain Cognitive Development (ABCD) study -filter:retweets\"\n",
    "search_query5 = \"ABCD sample -filter:retweets\"\n",
    "search_query6 = \"ABCD Study Site -filter:retweets\"\n",
    "search_query7 = \"#ABCD study -filter:retweets\"\n",
    "search_query8 = \"#abcdstudy -filter:retweets\"\n",
    "search_query9 = \"ABCD-BIDS Community Collection -filter:retweets\"\n",
    "search_query10 = \"NowIKnowMyABCD -filter:retweets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6e97e-8547-40cc-bd4d-897d97e4e0bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Common keywords in tweets related to ABCD \n",
    "- \"ABCD study -filter:retweets\"\n",
    "- \"Adolescent Brain Cognitive Development study -filter:retweets\" \n",
    "- \"Adolescent Brain Cognitive Development (ABCD) study -filter:retweets\"\n",
    "- \"ABCD sample -filter:retweets\"\n",
    "- \"ABCD Study Site -filter:retweets\"\n",
    "- \"#ABCD study -filter:retweets\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf694d3-5e06-42c5-98ca-65af61f625e4",
   "metadata": {},
   "source": [
    "### Getting tweets since 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "788faf66-e996-4c4b-a1e8-29994ef1d65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_71526/1046040303.py:15: FutureWarning: content is deprecated, use rawContent instead\n",
      "  csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import csv\n",
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets1.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('#ABCDstudy since:2022-08-03 until:2023-02-07 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8469e1c4-a724-44d8-8671-e455acdcd971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_71526/2033258337.py:13: FutureWarning: content is deprecated, use rawContent instead\n",
      "  csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n"
     ]
    }
   ],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets2.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('ABCD study since:2022-08-03 until:2023-02-07 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70bf9824-915e-4058-a65b-1097a4f160a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_71526/3638089602.py:13: FutureWarning: content is deprecated, use rawContent instead\n",
      "  csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n"
     ]
    }
   ],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets3.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('Adolescent Brain Cognitive Development study since:2022-08-03 until:2023-02-07 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa2c97f8-a64a-4cd2-9a45-040151488a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_71526/2052525525.py:13: FutureWarning: content is deprecated, use rawContent instead\n",
      "  csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n"
     ]
    }
   ],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets4.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('Adolescent Brain Cognitive Development (ABCD) study since:2022-08-03 until:2023-02-07 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd9ff517-7021-4c0a-82b8-2b36104bcc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_71526/377561205.py:13: FutureWarning: content is deprecated, use rawContent instead\n",
      "  csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n"
     ]
    }
   ],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets5.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('ABCD sample since:2022-08-03 until:2023-02-07 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0c7ab8d-35d2-47d6-bd2a-06136952c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets6.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('ABCD study site since:2022-08-03 until:2023-02-07 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbfab2ed-01c8-4ac2-85df-c6bcb3335f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_71526/2686394710.py:13: FutureWarning: content is deprecated, use rawContent instead\n",
      "  csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n"
     ]
    }
   ],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets7.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('#ABCD study since:2022-08-03 until:2023-02-07 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc99345f-501b-4595-b1fc-97313e8864c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_71526/2981362699.py:13: FutureWarning: content is deprecated, use rawContent instead\n",
      "  csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n"
     ]
    }
   ],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets8.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('#abcdstudy since:2022-08-03 until:2023-02-07 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e75b94ae-53ed-40de-adc8-28d709fa8da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets9.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('ABCD-BIDS Community Collection since:2022-08-03 until:2023-02-07 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eada1bb8-01a6-4b8a-8afb-c5dd96bbbd18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_71526/2837345454.py:13: FutureWarning: content is deprecated, use rawContent instead\n",
      "  csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n"
     ]
    }
   ],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets9.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('NowIKnowMyABCD since:2022-08-03 until:2023-02-07 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6137c90a-6350-46c0-afa3-f1dc4f597ad6",
   "metadata": {},
   "source": [
    "### Combining CSV files from output above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aaadbfab-8012-46a4-bdf9-3dcf276b420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory /Users/clare/Dropbox (University of Oregon)/Clare_McCann_files/Professional/tweets\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Working Directory\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43dff723-7aaf-4bf0-82b5-b391ed285f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "\n",
    "## set as repo current directoy\n",
    "\n",
    "os.chdir(\"/Users/clare/Dropbox (University of Oregon)/Clare_McCann_files/Professional/tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dccf228f-078a-417b-b82b-5b22065c4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('abcd_tweets*.{}'.format(extension))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83a6c827-aa82-4d17-897f-7aa7eea3614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "\n",
    "combined_csv.to_csv( \"combined_abcd_tweets.csv\", index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c3dbf02-bbe2-44d8-bb84-ecac7837ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in all_filenames:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d71007c0-335e-4fc1-9f1e-f8cf8477a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_old_abcd_tweets = pd.read_csv(\"combined_abcd_tweets.csv\")\n",
    "\n",
    "combined_old_abcd_tweets = combined_old_abcd_tweets.drop_duplicates()\n",
    "\n",
    "combined_old_abcd_tweets.to_csv(\"combined_abcd_tweets.csv\", index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940ea67-74b9-48ec-9bcd-9b33584a7a02",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ideas for sub-catergoies for the pulled tweets\n",
    "* \"published\", \"publications\", \"publication\", \"publish\", \"pre-print\", \"preprint\", \"paper\", \"papers\"\n",
    "* \"poster\", \"posters\", \"presented\", \"presentation\", \"present\", \"conference\"\n",
    "* \"workshop\", \"event\", \"events\", \"tutorial\", \"slides\", \"powerpoint\", \"curriculum\"\n",
    "* \"code\", \"troubleshoooting\", \"troubleshoot\", \"syntax\", \"variables\", \"variable\", \"issue\", \"issues\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3eef42a-1b51-4120-8151-3f93acf91fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      id         username                       date  \\\n",
      "0    1622599492892774401      PGCgenetics  2023-02-06 14:14:00+00:00   \n",
      "1    1612827483329671175           UM_PSC  2023-01-10 15:03:31+00:00   \n",
      "2    1608246106307321857        RuiyangGe  2022-12-28 23:38:46+00:00   \n",
      "3    1606342119114670081   URNeuroscience  2022-12-23 17:33:00+00:00   \n",
      "4    1603118784759631872         NIDAnews  2022-12-14 20:04:37+00:00   \n",
      "..                   ...              ...                        ...   \n",
      "356  1583466936024805377   neurology_live  2022-10-21 14:35:11+00:00   \n",
      "360  1570614185431801856          USCNIIN  2022-09-16 03:22:57+00:00   \n",
      "363  1570132204122181632    OrdoFibonacci  2022-09-14 19:27:43+00:00   \n",
      "365  1569969762360328197        ap_derfel  2022-09-14 08:42:14+00:00   \n",
      "378  1557868312972910599  biorxiv_neursci  2022-08-11 23:15:24+00:00   \n",
      "\n",
      "                                                 tweet  \\\n",
      "0    Study using #ADHD PRS from @PGCgenetics showed...   \n",
      "1    Using population-based neuroimaging data in yo...   \n",
      "2    our recent work in Molecular Psychiatry @molps...   \n",
      "3    12 Months of #Neuroscience #URochesterResearch...   \n",
      "4    üÜïNIDA-supported findings in @APADivision38 rei...   \n",
      "..                                                 ...   \n",
      "356  Findings from the recent Adolescent Brain Cogn...   \n",
      "360  Congrats to NIIN alumna Laura Ziemer on co-aut...   \n",
      "363  Association between mild traumatic brain injur...   \n",
      "365  Association between mild traumatic brain injur...   \n",
      "378  Fast, Efficient Multimodal Image Normalisation...   \n",
      "\n",
      "                                                   url  \n",
      "0    https://twitter.com/PGCgenetics/status/1622599...  \n",
      "1    https://twitter.com/UM_PSC/status/161282748332...  \n",
      "2    https://twitter.com/RuiyangGe/status/160824610...  \n",
      "3    https://twitter.com/URNeuroscience/status/1606...  \n",
      "4    https://twitter.com/NIDAnews/status/1603118784...  \n",
      "..                                                 ...  \n",
      "356  https://twitter.com/neurology_live/status/1583...  \n",
      "360  https://twitter.com/USCNIIN/status/15706141854...  \n",
      "363  https://twitter.com/OrdoFibonacci/status/15701...  \n",
      "365  https://twitter.com/ap_derfel/status/156996976...  \n",
      "378  https://twitter.com/biorxiv_neursci/status/155...  \n",
      "\n",
      "[233 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(combined_old_abcd_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26a88bc2-4df6-4d68-93d8-22c511c25ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_count = combined_old_abcd_tweets.tweet.str.split(expand=True).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "881567d6-e8a2-4431-a6c0-b8de538303cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_count = pd.DataFrame(data=text_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8fa416a1-f235-4856-806f-318f1c9463c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUE</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUSPENDED</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ONE</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#biorxiv_neursci</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2724 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "the               243\n",
       "and               193\n",
       "of                169\n",
       "in                149\n",
       "to                135\n",
       "...               ...\n",
       "DUE                 1\n",
       "SUSPENDED           1\n",
       "YEAR                1\n",
       "ONE                 1\n",
       "#biorxiv_neursci    1\n",
       "\n",
       "[2724 rows x 1 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "    \n",
    "text_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6f165-0714-4e5d-8845-da05ec173d9f",
   "metadata": {},
   "source": [
    "#### Getting tweets from the last week "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cea624e5-46f4-4c5d-8c4b-32d0c9f93455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweets from the API, might need to loop for various languages? \n",
    "\n",
    "## \"#ABCDstudy -filter:retweets\"\n",
    "\n",
    "tweets1 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query1,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets1_copy = []\n",
    "\n",
    "## \"ABCD study -filter:retweets\"\n",
    "\n",
    "tweets2 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query2,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets2_copy = []\n",
    "\n",
    "## \"Adolescent Brain Cognitive Development study -filter:retweets\"\n",
    "\n",
    "tweets3 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query3,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets3_copy = []\n",
    "\n",
    "## \"Adolescent Brain Cognitive Development (ABCD) study -filter:retweets\"\n",
    "\n",
    "tweets4 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query4,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets4_copy = []\n",
    "\n",
    "## \"ABCD sample -filter:retweets\"\n",
    "\n",
    "tweets5 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query5,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets5_copy = []\n",
    "\n",
    "## \"ABCD Study Site -filter:retweets\"\n",
    "\n",
    "tweets6 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query6,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets6_copy = []\n",
    "\n",
    "## \"#ABCD study -filter:retweets\"\n",
    "\n",
    "tweets7 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query7,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets7_copy = []\n",
    "\n",
    "## \"#abcdstudy -filter:retweets\"\n",
    "\n",
    "tweets8 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query8,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets8_copy = []\n",
    "\n",
    "## \"#ABCD-BIDS Community Collection -filter:retweets\"\n",
    "\n",
    "tweets9 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query9,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets9_copy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b57793b-e1a8-4d31-836d-d99ff8460afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tweets fetched: 1\n",
      "Total Tweets fetched: 17\n",
      "Total Tweets fetched: 4\n",
      "Total Tweets fetched: 4\n",
      "Total Tweets fetched: 2\n",
      "Total Tweets fetched: 1\n",
      "Total Tweets fetched: 2\n",
      "Total Tweets fetched: 1\n",
      "Total Tweets fetched: 0\n"
     ]
    }
   ],
   "source": [
    "# store the API responses in a list\n",
    "\n",
    "## \"#ABCDstudy -filter:retweets\"\n",
    "\n",
    "for tweet in tweets1:\n",
    "   tweets1_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets1_copy))\n",
    "\n",
    "## \"ABCD study -filter:retweets\"\n",
    "\n",
    "for tweet in tweets2:\n",
    "   tweets2_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets2_copy))\n",
    "\n",
    "## \"Adolescent Brain Cognitive Development study -filter:retweets\"\n",
    "\n",
    "for tweet in tweets3:\n",
    "   tweets3_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets3_copy))\n",
    "\n",
    "## \"Adolescent Brain Cognitive Development (ABCD) study -filter:retweets\"\n",
    "\n",
    "for tweet in tweets4:\n",
    "   tweets4_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets4_copy))\n",
    "\n",
    "## \"ABCD sample -filter:retweets\"\n",
    "\n",
    "for tweet in tweets5:\n",
    "   tweets5_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets5_copy))\n",
    "\n",
    "## \"ABCD Study Site -filter:retweets\"\n",
    "\n",
    "for tweet in tweets6:\n",
    "   tweets6_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets6_copy))\n",
    "\n",
    "## \"#ABCD study -filter:retweets\"\n",
    "\n",
    "for tweet in tweets7:\n",
    "   tweets7_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets7_copy))\n",
    "\n",
    "## \"#abcdstudy -filter:retweets\"\n",
    "\n",
    "for tweet in tweets8:\n",
    "   tweets8_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets8_copy))\n",
    "\n",
    "## \"ABCD-BIDS Community Collection -filter:retweets\"\n",
    "\n",
    "for tweet in tweets9:\n",
    "   tweets9_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets9_copy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "387b8c1a-b8ba-4990-83f1-26a4c55ea4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_90409/2002165555.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_90409/2002165555.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_90409/2002165555.py:56: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_90409/2002165555.py:76: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_90409/2002165555.py:96: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_90409/2002165555.py:116: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_90409/2002165555.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
      "/var/folders/2n/yr562b8j1mgcy9pvvfzzykmw0000gn/T/ipykernel_90409/2002165555.py:157: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n"
     ]
    }
   ],
   "source": [
    "# initialize the dataframe\n",
    "\n",
    "tweets_df = pd.DataFrame()\n",
    "\n",
    "# populate the dataframe\n",
    "\n",
    "for tweet in tweets1_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "for tweet in tweets2_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "for tweet in tweets3_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "for tweet in tweets4_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "\n",
    "for tweet in tweets5_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "for tweet in tweets6_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "\n",
    "for tweet in tweets7_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "for tweet in tweets8_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "for tweet in tweets9_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0dc22050-598d-4ebf-bc71-e547fdce459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      user_name      screen_name  \\\n",
      "0                             Dr. Tracy Riggins  TracyRigginsPhD   \n",
      "1                                  Aria ~~ üßò‚Äç‚ôÄÔ∏è   something_abcd   \n",
      "2                               Jacek Debiec üíôüíõ      DebiecJacek   \n",
      "3                                 Sylvia Gattas     SylviaGattas   \n",
      "4   The Lens: Ophthalmology Research Newsletter      TheLens_oph   \n",
      "5                           Brain Sciences MDPI    BrainSci_MDPI   \n",
      "6                                Holly Fletcher    hollyfletcher   \n",
      "7                                  PsyArXiv-bot      PsyArXivBot   \n",
      "8                                   VIRAJ DOVAL       DovalViraj   \n",
      "9                          bioRxiv Neuroscience  biorxiv_neursci   \n",
      "10                                      bioRxiv  biorxivpreprint   \n",
      "11                          Innovative Eye Care    EyeInnovative   \n",
      "12                                    NIMH_RDoC        NIMH_RDoC   \n",
      "13                Gunanidhi Sahu (‡¨ó‡≠Å‡¨£‡¨®‡¨ø‡¨ß‡¨ø ‡¨∏‡¨æ‡¨π‡≠Å)   gunanidhi_sahu   \n",
      "14                    Research Square Preprints     RS_Preprints   \n",
      "15                             Jason Nagata, MD     jasonmnagata   \n",
      "16                              Michael C Neale          mcneale   \n",
      "17                                saurabh patel     drspatel4384   \n",
      "18                              Jacek Debiec üíôüíõ      DebiecJacek   \n",
      "19                          Brain Sciences MDPI    BrainSci_MDPI   \n",
      "20                                 PsyArXiv-bot      PsyArXivBot   \n",
      "21                             Jason Nagata, MD     jasonmnagata   \n",
      "22                              Jacek Debiec üíôüíõ      DebiecJacek   \n",
      "23                          Brain Sciences MDPI    BrainSci_MDPI   \n",
      "24                                 PsyArXiv-bot      PsyArXivBot   \n",
      "25                             Jason Nagata, MD     jasonmnagata   \n",
      "26                                         CLaE          leafs_s   \n",
      "27                               Holly Fletcher    hollyfletcher   \n",
      "28                              Michael C Neale          mcneale   \n",
      "29                                    NIMH_RDoC        NIMH_RDoC   \n",
      "30                Gunanidhi Sahu (‡¨ó‡≠Å‡¨£‡¨®‡¨ø‡¨ß‡¨ø ‡¨∏‡¨æ‡¨π‡≠Å)   gunanidhi_sahu   \n",
      "31                            Dr. Tracy Riggins  TracyRigginsPhD   \n",
      "\n",
      "                user_location  \\\n",
      "0               Maryland, USA   \n",
      "1                               \n",
      "2                        ·º¶Œ∏ŒøœÇ   \n",
      "3                               \n",
      "4                               \n",
      "5          Basel, Switzerland   \n",
      "6               Nashville, TN   \n",
      "7                  Bot Heaven   \n",
      "8                               \n",
      "9                               \n",
      "10                   New York   \n",
      "11  Adelaide, South Australia   \n",
      "12              Maryland, USA   \n",
      "13                              \n",
      "14                 Durham, NC   \n",
      "15          San Francisco, CA   \n",
      "16              Virginia, USA   \n",
      "17                              \n",
      "18                       ·º¶Œ∏ŒøœÇ   \n",
      "19         Basel, Switzerland   \n",
      "20                 Bot Heaven   \n",
      "21          San Francisco, CA   \n",
      "22                       ·º¶Œ∏ŒøœÇ   \n",
      "23         Basel, Switzerland   \n",
      "24                 Bot Heaven   \n",
      "25          San Francisco, CA   \n",
      "26                              \n",
      "27              Nashville, TN   \n",
      "28              Virginia, USA   \n",
      "29              Maryland, USA   \n",
      "30                              \n",
      "31              Maryland, USA   \n",
      "\n",
      "                                     user_description  user_verified  \\\n",
      "0   PI Neurocognitive Development Lab @UMD.  Resea...          False   \n",
      "1   22 | She/Her | Indian | Software Engineer\\n\\n#...          False   \n",
      "2   Child, Adolescent & Adult Psychiatrist. Neuros...          False   \n",
      "3   PhD student @OxExpPsy. Interested in affective...          False   \n",
      "4   Weekly summaries of the most important new #re...          False   \n",
      "5   Brain Sciences (IF3.333, ISSN 2076-3425; in Pu...          False   \n",
      "6   ideas + words | experimented with @readbirddog...          False   \n",
      "7   I am your bot for @PsyArXiv psychology preprin...          False   \n",
      "8   Time,  Never Let Over Come A Haunted Past Expe...          False   \n",
      "9                                                              False   \n",
      "10  The non-profit preprint server for the life sc...          False   \n",
      "11  The optometrists at Innovative Eye Care strive...          False   \n",
      "12  Official Twitter account of the NIMH Research ...           True   \n",
      "13  Life is Beautiful.! Just keep Exploring.! Prou...          False   \n",
      "14  Making research communication faster, fairer, ...          False   \n",
      "15  Assistant Professor of Pediatrics & Adolescent...          False   \n",
      "16  Father of six, statistical geneticist, profess...          False   \n",
      "17         NotAtIMEwASTER, SURGEON, BUDDING INVESTOR,          False   \n",
      "18  Child, Adolescent & Adult Psychiatrist. Neuros...          False   \n",
      "19  Brain Sciences (IF3.333, ISSN 2076-3425; in Pu...          False   \n",
      "20  I am your bot for @PsyArXiv psychology preprin...          False   \n",
      "21  Assistant Professor of Pediatrics & Adolescent...          False   \n",
      "22  Child, Adolescent & Adult Psychiatrist. Neuros...          False   \n",
      "23  Brain Sciences (IF3.333, ISSN 2076-3425; in Pu...          False   \n",
      "24  I am your bot for @PsyArXiv psychology preprin...          False   \n",
      "25  Assistant Professor of Pediatrics & Adolescent...          False   \n",
      "26  Neuroscience,Insular cortex, Neurobiology of g...          False   \n",
      "27  ideas + words | experimented with @readbirddog...          False   \n",
      "28  Father of six, statistical geneticist, profess...          False   \n",
      "29  Official Twitter account of the NIMH Research ...           True   \n",
      "30  Life is Beautiful.! Just keep Exploring.! Prou...          False   \n",
      "31  PI Neurocognitive Development Lab @UMD.  Resea...          False   \n",
      "\n",
      "                        date  \\\n",
      "0  2022-08-02 21:32:03+00:00   \n",
      "1  2022-08-04 15:48:18+00:00   \n",
      "2  2022-08-03 21:12:45+00:00   \n",
      "3  2022-08-02 11:47:21+00:00   \n",
      "4  2022-08-01 12:00:03+00:00   \n",
      "5  2022-08-01 10:00:00+00:00   \n",
      "6  2022-07-29 17:57:09+00:00   \n",
      "7  2022-07-29 17:37:07+00:00   \n",
      "8  2022-07-29 13:48:58+00:00   \n",
      "9  2022-07-28 18:16:39+00:00   \n",
      "10 2022-07-28 18:16:39+00:00   \n",
      "11 2022-07-28 00:00:10+00:00   \n",
      "12 2022-07-27 19:13:28+00:00   \n",
      "13 2022-07-27 01:50:30+00:00   \n",
      "14 2022-07-26 17:58:05+00:00   \n",
      "15 2022-07-26 16:17:39+00:00   \n",
      "16 2022-07-26 14:13:35+00:00   \n",
      "17 2022-07-26 12:03:03+00:00   \n",
      "18 2022-08-03 21:12:45+00:00   \n",
      "19 2022-08-01 10:00:00+00:00   \n",
      "20 2022-07-29 17:37:07+00:00   \n",
      "21 2022-07-26 16:17:39+00:00   \n",
      "22 2022-08-03 21:12:45+00:00   \n",
      "23 2022-08-01 10:00:00+00:00   \n",
      "24 2022-07-29 17:37:07+00:00   \n",
      "25 2022-07-26 16:17:39+00:00   \n",
      "26 2022-07-30 15:24:39+00:00   \n",
      "27 2022-07-29 17:57:09+00:00   \n",
      "28 2022-07-26 14:13:35+00:00   \n",
      "29 2022-07-27 19:13:28+00:00   \n",
      "30 2022-07-27 01:50:30+00:00   \n",
      "31 2022-08-02 21:32:03+00:00   \n",
      "\n",
      "                                                 text  \\\n",
      "0   @morganbotdorf Using ABCD data -- greater soci...   \n",
      "1   @197CMTAE Our study boooks should also have au...   \n",
      "2   The ongoing Adolescent Brain Cognitive Develop...   \n",
      "3   The University of Oxford's ABCD lab is looking...   \n",
      "4   A recent study evaluated the sensitivity and s...   \n",
      "5   #mdpibrainsci Parental Education and Youth Inh...   \n",
      "6   Amplify -&gt; \\nthis @morganbotdorf @TracyRigg...   \n",
      "7   Characterizing the dimensional structure of ea...   \n",
      "8   @adarpoonawalla ji üôèüèª if musk can't negotiate ...   \n",
      "9   Acculturative orientations among Hispanic/Lati...   \n",
      "10  Acculturative orientations among Hispanic/Lati...   \n",
      "11  https://t.co/ENDoiWdq9U https://t.co/x0EI9k7SL...   \n",
      "12  Using #ABCD study data, @LaikaAguinaldo &amp; ...   \n",
      "13  Two year young Priyansh is studing ABCD #ABCD ...   \n",
      "14  Multiple Measurement Analysis of Resting-State...   \n",
      "15  üôè Many thanks to co-first author @jontchu and ...   \n",
      "16  @RicardJocelyn @Yale @BlackInNeuro We have a l...   \n",
      "17  @warikoo Since our nursery and LKG we having o...   \n",
      "18  The ongoing Adolescent Brain Cognitive Develop...   \n",
      "19  #mdpibrainsci Parental Education and Youth Inh...   \n",
      "20  Characterizing the dimensional structure of ea...   \n",
      "21  üôè Many thanks to co-first author @jontchu and ...   \n",
      "22  The ongoing Adolescent Brain Cognitive Develop...   \n",
      "23  #mdpibrainsci Parental Education and Youth Inh...   \n",
      "24  Characterizing the dimensional structure of ea...   \n",
      "25  üôè Many thanks to co-first author @jontchu and ...   \n",
      "26  Developmental Cognitive Neuroscience\\n\\nSocioe...   \n",
      "27  Amplify -&gt; \\nthis @morganbotdorf @TracyRigg...   \n",
      "28  @RicardJocelyn @Yale @BlackInNeuro We have a l...   \n",
      "29  Using #ABCD study data, @LaikaAguinaldo &amp; ...   \n",
      "30  Two year young Priyansh is studing ABCD #ABCD ...   \n",
      "31  @morganbotdorf Using ABCD data -- greater soci...   \n",
      "\n",
      "                                                  url  \\\n",
      "0   https://twitter.com/TracyRigginsPhD/status/155...   \n",
      "1   https://twitter.com/something_abcd/status/1555...   \n",
      "2   https://twitter.com/DebiecJacek/status/1554938...   \n",
      "3   https://twitter.com/SylviaGattas/status/155443...   \n",
      "4   https://twitter.com/TheLens_oph/status/1554074...   \n",
      "5   https://twitter.com/BrainSci_MDPI/status/15540...   \n",
      "6   https://twitter.com/hollyfletcher/status/15530...   \n",
      "7   https://twitter.com/PsyArXivBot/status/1553072...   \n",
      "8   https://twitter.com/DovalViraj/status/15530147...   \n",
      "9   https://twitter.com/biorxiv_neursci/status/155...   \n",
      "10  https://twitter.com/biorxivpreprint/status/155...   \n",
      "11  https://twitter.com/EyeInnovative/status/15524...   \n",
      "12  https://twitter.com/NIMH_RDoC/status/155237161...   \n",
      "13  https://twitter.com/gunanidhi_sahu/status/1552...   \n",
      "14  https://twitter.com/RS_Preprints/status/155199...   \n",
      "15  https://twitter.com/jasonmnagata/status/155196...   \n",
      "16  https://twitter.com/mcneale/status/15519337557...   \n",
      "17  https://twitter.com/drspatel4384/status/155190...   \n",
      "18  https://twitter.com/DebiecJacek/status/1554938...   \n",
      "19  https://twitter.com/BrainSci_MDPI/status/15540...   \n",
      "20  https://twitter.com/PsyArXivBot/status/1553072...   \n",
      "21  https://twitter.com/jasonmnagata/status/155196...   \n",
      "22  https://twitter.com/DebiecJacek/status/1554938...   \n",
      "23  https://twitter.com/BrainSci_MDPI/status/15540...   \n",
      "24  https://twitter.com/PsyArXivBot/status/1553072...   \n",
      "25  https://twitter.com/jasonmnagata/status/155196...   \n",
      "26  https://twitter.com/leafs_s/status/15534011930...   \n",
      "27  https://twitter.com/hollyfletcher/status/15530...   \n",
      "28  https://twitter.com/mcneale/status/15519337557...   \n",
      "29  https://twitter.com/NIMH_RDoC/status/155237161...   \n",
      "30  https://twitter.com/gunanidhi_sahu/status/1552...   \n",
      "31  https://twitter.com/TracyRigginsPhD/status/155...   \n",
      "\n",
      "                                             hashtags               source  \n",
      "0                                                None      Twitter Web App  \n",
      "1                                                None  Twitter for Android  \n",
      "2                                                None      Twitter Web App  \n",
      "3                                                None      Twitter Web App  \n",
      "4                                                None      Twitter Web App  \n",
      "5                                      [mdpibrainsci]            TweetDeck  \n",
      "6                                                None      Twitter Web App  \n",
      "7                                                None           Zapier.com  \n",
      "8                                                None  Twitter for Android  \n",
      "9                                                None      biorxiv_neursci  \n",
      "10                                               None      biorxiv_connect  \n",
      "11                                               None       CAPTIV8 Social  \n",
      "12                                       [ABCD, RDoC]      Twitter Web App  \n",
      "13  [ABCD, TWOyearKIDSabcd, babyfunnyvideo, Learni...  Twitter for Android  \n",
      "14                                               None         rs-preprints  \n",
      "15                                               None      Twitter Web App  \n",
      "16                                               None     Twitter for iPad  \n",
      "17                                               None  Twitter for Android  \n",
      "18                                               None      Twitter Web App  \n",
      "19                                     [mdpibrainsci]            TweetDeck  \n",
      "20                                               None           Zapier.com  \n",
      "21                                               None      Twitter Web App  \n",
      "22                                               None      Twitter Web App  \n",
      "23                                     [mdpibrainsci]            TweetDeck  \n",
      "24                                               None           Zapier.com  \n",
      "25                                               None      Twitter Web App  \n",
      "26                                               None   Twitter for iPhone  \n",
      "27                                               None      Twitter Web App  \n",
      "28                                               None     Twitter for iPad  \n",
      "29                                       [ABCD, RDoC]      Twitter Web App  \n",
      "30  [ABCD, TWOyearKIDSabcd, babyfunnyvideo, Learni...  Twitter for Android  \n",
      "31                                               None      Twitter Web App  \n"
     ]
    }
   ],
   "source": [
    "print(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c300286c-f68b-4e6d-a81c-3d977c4828fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('abcd_past_week_tweets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
