{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b524377-4625-411e-a709-fd101d1f5f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (4.10.0)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from tweepy) (2.27.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from tweepy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tweepy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a61d9a52-6363-48d6-8109-4f6c6c577866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (0.4.3.20220106)\n",
      "Requirement already satisfied: pytz in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from snscrape) (2022.1)\n",
      "Requirement already satisfied: lxml in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from snscrape) (4.9.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from snscrape) (4.10.0)\n",
      "Requirement already satisfied: requests[socks] in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from snscrape) (2.27.1)\n",
      "Requirement already satisfied: filelock in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from snscrape) (3.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from beautifulsoup4->snscrape) (2.3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests[socks]->snscrape) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests[socks]->snscrape) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests[socks]->snscrape) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests[socks]->snscrape) (1.26.9)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/clare/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ea16f3-47e6-4527-8a32-42942419fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tweepy\n",
    "\n",
    "import tweepy as tw\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# your Twitter API key and API secret\n",
    "\n",
    "consumer_key = \"XXXXXXXXXXXXXX\"\n",
    "consumer_secret = \"XXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "access_key = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "access_secret = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "\n",
    "# authenticate\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tw.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538f210b-3407-4de0-b894-5b600f65fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query1 = \"#ABCDstudy -filter:retweets\"\n",
    "search_query2 = \"ABCD study -filter:retweets\"\n",
    "search_query3 = \"Adolescent Brain Cognitive Development study -filter:retweets\"\n",
    "search_query4 = \"Adolescent Brain Cognitive Development (ABCD) study -filter:retweets\"\n",
    "search_query5 = \"ABCD sample -filter:retweets\"\n",
    "search_query6 = \"ABCD Study Site -filter:retweets\"\n",
    "search_query7 = \"#ABCD study -filter:retweets\"\n",
    "search_query8 = \"#abcdstudy -filter:retweets\"\n",
    "search_query9 = \"#ABCC -filter:retweets\"\n",
    "search_query10 = \"ABCD-BIDS Community Collection -filter:retweets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6e97e-8547-40cc-bd4d-897d97e4e0bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Common keywords in tweets related to ABCD \n",
    "- \"ABCD study -filter:retweets\"\n",
    "- \"Adolescent Brain Cognitive Development study -filter:retweets\" \n",
    "- \"Adolescent Brain Cognitive Development (ABCD) study -filter:retweets\"\n",
    "- \"ABCD sample -filter:retweets\"\n",
    "- \"ABCD Study Site -filter:retweets\"\n",
    "- \"#ABCD study -filter:retweets\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf694d3-5e06-42c5-98ca-65af61f625e4",
   "metadata": {},
   "source": [
    "### Getting tweets since 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788faf66-e996-4c4b-a1e8-29994ef1d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import csv\n",
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets1.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('#ABCDstudy since:2015-01-01 until:2022-08-03 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8469e1c4-a724-44d8-8671-e455acdcd971",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets2.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('ABCD study since:2015-01-01 until:2022-08-03 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70bf9824-915e-4058-a65b-1097a4f160a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets3.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('Adolescent Brain Cognitive Development study since:2015-01-01 until:2022-08-03 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa2c97f8-a64a-4cd2-9a45-040151488a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets4.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('Adolescent Brain Cognitive Development (ABCD) study since:2015-01-01 until:2022-08-03 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd9ff517-7021-4c0a-82b8-2b36104bcc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets5.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('ABCD sample since:2015-01-01 until:2022-08-03 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0c7ab8d-35d2-47d6-bd2a-06136952c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets6.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('ABCD study site since:2015-01-01 until:2022-08-03 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbfab2ed-01c8-4ac2-85df-c6bcb3335f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets7.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('#ABCD study since:2015-01-01 until:2022-08-03 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc99345f-501b-4595-b1fc-97313e8864c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets8.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('#abcdstudy since:2015-01-01 until:2022-08-03 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c95ca7ce-6e90-4883-a004-ccd9cf69fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets9.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('#ABCC since:2015-01-01 until:2022-08-03 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e75b94ae-53ed-40de-adc8-28d709fa8da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 3000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('abcd_tweets10.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id', 'username', 'date','tweet', 'url']) \n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('ABCD-BIDS Community Collection since:2015-01-01 until:2022-08-03 lang:en -filter:replies -filter:retweets').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break  \n",
    "    csvWriter.writerow([tweet.id, tweet.user.username, tweet.date, tweet.content, tweet.url])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6137c90a-6350-46c0-afa3-f1dc4f597ad6",
   "metadata": {},
   "source": [
    "### Combining CSV files from output above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaadbfab-8012-46a4-bdf9-3dcf276b420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory /Users/clare/Desktop/tweets\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Working Directory\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43dff723-7aaf-4bf0-82b5-b391ed285f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "\n",
    "## set as repo current directoy\n",
    "\n",
    "os.chdir(\"/Users/clare/Desktop/tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dccf228f-078a-417b-b82b-5b22065c4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('abcd_tweets*.{}'.format(extension))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83a6c827-aa82-4d17-897f-7aa7eea3614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "\n",
    "combined_csv.to_csv( \"combined_abcd_tweets.csv\", index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c3dbf02-bbe2-44d8-bb84-ecac7837ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in all_filenames:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d71007c0-335e-4fc1-9f1e-f8cf8477a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_old_abcd_tweets = pd.read_csv(\"combined_abcd_tweets.csv\")\n",
    "\n",
    "combined_old_abcd_tweets = combined_old_abcd_tweets.drop_duplicates()\n",
    "\n",
    "combined_old_abcd_tweets.to_csv(\"combined_abcd_tweets.csv\", index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940ea67-74b9-48ec-9bcd-9b33584a7a02",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ideas for sub-catergoies for the pulled tweets\n",
    "* \"published\", \"publications\", \"publication\", \"publish\", \"pre-print\", \"preprint\", \"paper\", \"papers\"\n",
    "* \"poster\", \"posters\", \"presented\", \"presentation\", \"present\", \"conference\"\n",
    "* \"workshop\", \"event\", \"events\", \"tutorial\", \"slides\", \"powerpoint\", \"curriculum\"\n",
    "* \"code\", \"troubleshoooting\", \"troubleshoot\", \"syntax\", \"variables\", \"variable\", \"issue\", \"issues\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3eef42a-1b51-4120-8151-3f93acf91fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       id       username                       date  \\\n",
      "0     1541865217386323976   UHHealthFast  2022-06-28 19:24:48+00:00   \n",
      "1     1541836326110081024         JAACAP  2022-06-28 17:30:00+00:00   \n",
      "2     1541730628617519105         JAACAP  2022-06-28 10:30:00+00:00   \n",
      "3     1541443038492479490   Ran_BarziLab  2022-06-27 15:27:13+00:00   \n",
      "4     1538877776450752512  Sam_D_Parsons  2022-06-20 13:33:47+00:00   \n",
      "...                   ...            ...                        ...   \n",
      "7850   647532329645289472     BioNorthTX  2015-09-25 22:05:12+00:00   \n",
      "7896   608643106167087104    WorleyJulie  2015-06-10 14:33:19+00:00   \n",
      "7910   593807568373358592        ASAMorg  2015-04-30 16:02:11+00:00   \n",
      "7924   584075275639595009        ATP_CME  2015-04-03 19:29:31+00:00   \n",
      "7925   580023871786598401   NINDSfunding  2015-03-23 15:10:42+00:00   \n",
      "\n",
      "                                                  tweet  \\\n",
      "0     On 6/28/2022, Dr. Natasha Wade from @UCSanDieg...   \n",
      "1     Large-scale study of youth provides some of th...   \n",
      "2     Large-scale study of youth provides some of th...   \n",
      "3     Our paper on cyberbullying &amp; suicidality i...   \n",
      "4     Has anyone had success mapping the white matte...   \n",
      "...                                                 ...   \n",
      "7850  Grant Awards Mark the Launch of Landmark Adole...   \n",
      "7896  Study of Adolescent Brain Cognitive Developmen...   \n",
      "7910  Adolescent Brain Cognitive Development Study t...   \n",
      "7924  Top story: Longitudinal Study of Adolescent Br...   \n",
      "7925  #NIH clarifies Adolescent Brain Cognitive Deve...   \n",
      "\n",
      "                                                    url  \n",
      "0     https://twitter.com/UHHealthFast/status/154186...  \n",
      "1     https://twitter.com/JAACAP/status/154183632611...  \n",
      "2     https://twitter.com/JAACAP/status/154173062861...  \n",
      "3     https://twitter.com/Ran_BarziLab/status/154144...  \n",
      "4     https://twitter.com/Sam_D_Parsons/status/15388...  \n",
      "...                                                 ...  \n",
      "7850  https://twitter.com/BioNorthTX/status/64753232...  \n",
      "7896  https://twitter.com/WorleyJulie/status/6086431...  \n",
      "7910  https://twitter.com/ASAMorg/status/59380756837...  \n",
      "7924  https://twitter.com/ATP_CME/status/58407527563...  \n",
      "7925  https://twitter.com/NINDSfunding/status/580023...  \n",
      "\n",
      "[5782 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(combined_old_abcd_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5e1604-ea9c-45fd-9feb-f2225c982670",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ranking most popular words \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "new_headers = ['tweet']\n",
    "\n",
    "for_filter = pd.DataFrame(combined_old_abcd_tweets, columns=new_headers)\n",
    "\n",
    "for_filter\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208eb33-fc8a-4b8c-9837-ea064dd7e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_filter.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9a32a-3b9e-488a-9f8e-78378f652384",
   "metadata": {},
   "outputs": [],
   "source": [
    "## most common words \n",
    "\n",
    "for_filter.tweet = str(tweet)\n",
    "\n",
    "common = pd.Series(' '.join(for_filter['tweet']).split()).value_counts()[:40]\n",
    "\n",
    "common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6f165-0714-4e5d-8845-da05ec173d9f",
   "metadata": {},
   "source": [
    "#### Getting tweets from the last week "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea624e5-46f4-4c5d-8c4b-32d0c9f93455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweets from the API, might need to loop for various languages? \n",
    "\n",
    "## \"#ABCDstudy -filter:retweets\"\n",
    "\n",
    "tweets1 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query1,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets1_copy = []\n",
    "\n",
    "## \"ABCD study -filter:retweets\"\n",
    "\n",
    "tweets2 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query2,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets2_copy = []\n",
    "\n",
    "## \"Adolescent Brain Cognitive Development study -filter:retweets\"\n",
    "\n",
    "tweets3 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query3,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets3_copy = []\n",
    "\n",
    "## \"Adolescent Brain Cognitive Development (ABCD) study -filter:retweets\"\n",
    "\n",
    "tweets4 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query4,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets4_copy = []\n",
    "\n",
    "## \"ABCD sample -filter:retweets\"\n",
    "\n",
    "tweets5 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query5,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets5_copy = []\n",
    "\n",
    "## \"ABCD Study Site -filter:retweets\"\n",
    "\n",
    "tweets6 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query6,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets6_copy = []\n",
    "\n",
    "## \"#ABCD study -filter:retweets\"\n",
    "\n",
    "tweets7 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query7,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets7_copy = []\n",
    "\n",
    "## \"#abcdstudy -filter:retweets\"\n",
    "\n",
    "tweets8 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query8,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets8_copy = []\n",
    "\n",
    "## \"#ABCC -filter:retweets\"\n",
    "\n",
    "tweets9 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query9,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets9_copy = []\n",
    "\n",
    "## \"#ABCD-BIDS Community Collection -filter:retweets\"\n",
    "\n",
    "tweets10 = tw.Cursor(api.search_tweets,\n",
    "             q=search_query10,\n",
    "             lang=\"en\").items(10000)\n",
    "\n",
    "tweets10_copy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57793b-e1a8-4d31-836d-d99ff8460afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the API responses in a list\n",
    "\n",
    "## \"#ABCDstudy -filter:retweets\"\n",
    "\n",
    "for tweet in tweets1:\n",
    "   tweets1_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets1_copy))\n",
    "\n",
    "## \"ABCD study -filter:retweets\"\n",
    "\n",
    "for tweet in tweets2:\n",
    "   tweets2_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets2_copy))\n",
    "\n",
    "## \"Adolescent Brain Cognitive Development study -filter:retweets\"\n",
    "\n",
    "for tweet in tweets3:\n",
    "   tweets3_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets3_copy))\n",
    "\n",
    "## \"Adolescent Brain Cognitive Development (ABCD) study -filter:retweets\"\n",
    "\n",
    "for tweet in tweets4:\n",
    "   tweets4_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets4_copy))\n",
    "\n",
    "## \"ABCD sample -filter:retweets\"\n",
    "\n",
    "for tweet in tweets5:\n",
    "   tweets5_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets5_copy))\n",
    "\n",
    "## \"ABCD Study Site -filter:retweets\"\n",
    "\n",
    "for tweet in tweets6:\n",
    "   tweets6_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets6_copy))\n",
    "\n",
    "## \"#ABCD study -filter:retweets\"\n",
    "\n",
    "for tweet in tweets7:\n",
    "   tweets7_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets7_copy))\n",
    "\n",
    "## \"#abcdstudy -filter:retweets\"\n",
    "\n",
    "for tweet in tweets8:\n",
    "   tweets8_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets8_copy))\n",
    "\n",
    "## \"#ABCC -filter:retweets\"\n",
    "\n",
    "for tweet in tweets9:\n",
    "   tweets9_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets9_copy))\n",
    "\n",
    "## \"ABCD-BIDS Community Collection -filter:retweets\"\n",
    "\n",
    "for tweet in tweets10:\n",
    "   tweets10_copy.append(tweet)\n",
    "\n",
    "print(\"Total Tweets fetched:\", len(tweets10_copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b8c1a-b8ba-4990-83f1-26a4c55ea4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dataframe\n",
    "\n",
    "tweets_df = pd.DataFrame()\n",
    "\n",
    "# populate the dataframe\n",
    "\n",
    "for tweet in tweets1_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "for tweet in tweets2_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "for tweet in tweets3_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "for tweet in tweets4_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "\n",
    "for tweet in tweets5_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "for tweet in tweets6_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "\n",
    "for tweet in tweets7_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "for tweet in tweets8_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "for tweet in tweets9_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "for tweet in tweets10_copy:\n",
    "   hashtags = []\n",
    "   try:\n",
    "       for hashtag in tweet.entities[\"hashtags\"]:\n",
    "           hashtags.append(hashtag[\"text\"])\n",
    "       text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "   except:\n",
    "       pass\n",
    "   tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name,\n",
    "                                              'screen_name': tweet.user.screen_name,\n",
    "                                              'user_location': tweet.user.location,\\\n",
    "                                              'user_description': tweet.user.description,\n",
    "                                              'user_verified': tweet.user.verified,\n",
    "                                              'date': tweet.created_at,\n",
    "                                              'text': text,\n",
    "                                              'url':f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
    "                                              'hashtags': [hashtags if hashtags else None],\n",
    "                                              'source': tweet.source}))\n",
    "   tweets_df = tweets_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc22050-598d-4ebf-bc71-e547fdce459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300286c-f68b-4e6d-a81c-3d977c4828fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('abcd_past_week_tweets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
